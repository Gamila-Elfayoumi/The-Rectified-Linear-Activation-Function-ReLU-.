# The-Rectified-Linear-Activation-Function-ReLU.
# is a standard in both industry and research.
# and has been shown to lead to very high-performance networks. 
This function takes a single number as an input, returning 0 if the input is negative, and the input if the input is positive.
Here are some examples:
relu(3) = 3
relu(-3) = 0
•	Apply the relu() function to node_0_input to calculate node_0_output.
•	Apply the relu() function to node_1_input to calculate node_1_output.
o	Using the max() function to calculate the value for the output of relu().
